<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Value Iteration | Chan`s Jupyter</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Value Iteration" />
<meta name="author" content="GitHub User" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A summary of “Understanding deep reinforcement learning”" />
<meta property="og:description" content="A summary of “Understanding deep reinforcement learning”" />
<link rel="canonical" href="https://goodboychan.github.io/reinforcement_learning/2020/06/09/04-Value-Iteration.html" />
<meta property="og:url" content="https://goodboychan.github.io/reinforcement_learning/2020/06/09/04-Value-Iteration.html" />
<meta property="og:site_name" content="Chan`s Jupyter" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-09T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://goodboychan.github.io/reinforcement_learning/2020/06/09/04-Value-Iteration.html","headline":"Value Iteration","dateModified":"2020-06-09T00:00:00-05:00","datePublished":"2020-06-09T00:00:00-05:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://goodboychan.github.io/reinforcement_learning/2020/06/09/04-Value-Iteration.html"},"author":{"@type":"Person","name":"GitHub User"},"description":"A summary of “Understanding deep reinforcement learning”","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-33905785-1', 'auto');
    ga('send', 'pageview');
  </script>



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="https://goodboychan.github.io/">Chan`s Jupyter</a></h1>

        

        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>

        

        

        
      </header>
      <section>

      <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Value Iteration</h1><p class="page-description">A summary of "Understanding deep reinforcement learning"</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-06-09T00:00:00-05:00" itemprop="datePublished">
        Jun 9, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      1 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#Reinforcement_Learning">Reinforcement_Learning</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#policy-improvement-by-iterative-methods">Policy Improvement by Iterative Methods</a>
<ul>
<li class="toc-entry toc-h2"><a href="#value-iteration">Value Iteration</a></li>
</ul>
</li>
</ul><h1 id="policy-improvement-by-iterative-methods">
<a class="anchor" href="#policy-improvement-by-iterative-methods" aria-hidden="true"><span class="octicon octicon-link"></span></a>Policy Improvement by Iterative Methods</h1>

<h2 id="value-iteration">
<a class="anchor" href="#value-iteration" aria-hidden="true"><span class="octicon octicon-link"></span></a>Value Iteration</h2>
<ul>
  <li>Initialize state-value for all states</li>
  <li>Iterate for $k$ until convergence
    <ul>
      <li>Update the state-value for all states</li>
    </ul>
  </li>
</ul>

<p>$V_{k+1} (s) = \max_{a \in A}(R(s, a) + \gamma \sum_{s’ \in S} P(s’ \vert s, a) V_k(s’))$</p>

<p>We can re-define this term with expectation,</p>

<p>$V_{k+1} (s) = \max_{a \in A}(R(s, a) + \gamma E_k[Q_k(s_{t+1}, a) \vert s_t = s])$</p>

<ul>
  <li>Iterative application of Bellman  optimality backup</li>
  <li>Using tensor notation ($E_k[Q_k(s_{t+1}, a) \vert s_t = s] \rarr \Tau^a V_k$):</li>
</ul>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>V</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><mrow><mi>a</mi><mo>∈</mo><mi>A</mi></mrow></munder><mo stretchy="false">(</mo><mi>R</mi><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo><mo>+</mo><mi>γ</mi><msup><mi mathvariant="normal">T</mi><mi>a</mi></msup><msub><mi>V</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V_{k+1} = \max_{a \in A}(R(a) + \gamma \Tau^a V_k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8917em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.5217em;vertical-align:-0.7717em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em;"><span style="top:-2.3557em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight">A</span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop">max</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7717em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mord"><span class="mord mathrm">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7144em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>

<ul>
  <li>
    <p>This is called synchronous backup. The reason we called “backup” is that, we always hold state value in current time step and next time step. And when the state value in current time step is prepared, then we can calculate the next time step state value. So it is synchronous.</p>
  </li>
  <li>Value iteration finds no explicit policy at each iteration, even the optimal policy.</li>
  <li>To find the optimal policy, we should save and update the best action for each iteration.</li>
  <li>The complexity is $O(\vert A \vert \vert S \vert^2)$ per iteration.
($\vert S \vert$ for current state, and $\vert S \vert$ for next state)</li>
</ul>

  </div><a class="u-url" href="/reinforcement_learning/2020/06/09/04-Value-Iteration.html" hidden></a>
</article>

      </section>
      <footer>
        
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
  </body>
</html>
