<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Classification with XGBoost | Chan`s Jupyter</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Classification with XGBoost" />
<meta name="author" content="Chanseok Kang" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This chapter will introduce you to the fundamental idea behind XGBoost—boosted learners. Once you understand how XGBoost works, you’ll apply it to solve a common classification problem found in industry - predicting whether a customer will stop being a customer at some point in the future. This is the Summary of lecture “Extreme Gradient Boosting with XGBoost”, via datacamp." />
<meta property="og:description" content="This chapter will introduce you to the fundamental idea behind XGBoost—boosted learners. Once you understand how XGBoost works, you’ll apply it to solve a common classification problem found in industry - predicting whether a customer will stop being a customer at some point in the future. This is the Summary of lecture “Extreme Gradient Boosting with XGBoost”, via datacamp." />
<link rel="canonical" href="https://goodboychan.github.io/python/datacamp/machine_learning/2020/07/06/03-Classification-with-XGBoost.html" />
<meta property="og:url" content="https://goodboychan.github.io/python/datacamp/machine_learning/2020/07/06/03-Classification-with-XGBoost.html" />
<meta property="og:site_name" content="Chan`s Jupyter" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-06T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://goodboychan.github.io/python/datacamp/machine_learning/2020/07/06/03-Classification-with-XGBoost.html","headline":"Classification with XGBoost","dateModified":"2020-07-06T00:00:00-05:00","datePublished":"2020-07-06T00:00:00-05:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://goodboychan.github.io/python/datacamp/machine_learning/2020/07/06/03-Classification-with-XGBoost.html"},"author":{"@type":"Person","name":"Chanseok Kang"},"description":"This chapter will introduce you to the fundamental idea behind XGBoost—boosted learners. Once you understand how XGBoost works, you’ll apply it to solve a common classification problem found in industry - predicting whether a customer will stop being a customer at some point in the future. This is the Summary of lecture “Extreme Gradient Boosting with XGBoost”, via datacamp.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-33905785-1', 'auto');
    ga('send', 'pageview');
  </script>



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="https://goodboychan.github.io/">Chan`s Jupyter</a></h1>

        

        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>

        

        

        
      </header>
      <section>

      <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Classification with XGBoost</h1><p class="page-description">This chapter will introduce you to the fundamental idea behind XGBoost—boosted learners. Once you understand how XGBoost works, you'll apply it to solve a common classification problem found in industry - predicting whether a customer will stop being a customer at some point in the future. This is the Summary of lecture "Extreme Gradient Boosting with XGBoost", via datacamp.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-07-06T00:00:00-05:00" itemprop="datePublished">
        Jul 6, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Chanseok Kang</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#Python">Python</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#Datacamp">Datacamp</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#Machine_Learning">Machine_Learning</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/goodboychan/goodboychan.github.io/tree/main/_notebooks/2020-07-06-03-Classification-with-XGBoost.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/goodboychan/goodboychan.github.io/main?filepath=_notebooks%2F2020-07-06-03-Classification-with-XGBoost.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/goodboychan/goodboychan.github.io/blob/main/_notebooks/2020-07-06-03-Classification-with-XGBoost.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fgoodboychan%2Fgoodboychan.github.io%2Fblob%2Fmain%2F_notebooks%2F2020-07-06-03-Classification-with-XGBoost.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Introduction">Introduction </a></li>
<li class="toc-entry toc-h2"><a href="#Introducing-XGBoost">Introducing XGBoost </a>
<ul>
<li class="toc-entry toc-h3"><a href="#XGBoost---Fit/Predict">XGBoost - Fit/Predict </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#What-is-a-decision-tree?">What is a decision tree? </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Decision-trees">Decision trees </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#What-is-Boosting?">What is Boosting? </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Measuring-accuracy">Measuring accuracy </a></li>
<li class="toc-entry toc-h3"><a href="#Measuring-AUC">Measuring AUC </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#When-should-I-use-XGBoost?">When should I use XGBoost? </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-07-06-03-Classification-with-XGBoost.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Introduction">
<a class="anchor" href="#Introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction<a class="anchor-link" href="#Introduction"> </a>
</h2>
<ul>
<li>Supervised Learning<ul>
<li>Relies on labeled data</li>
<li>Have some understanding of past behavior</li>
</ul>
</li>
<li>AUC: Metric for binary classification models<ul>
<li>Area Under the ROC Curve (AUC)<ul>
<li>Larger area under the ROC curve = better model</li>
</ul>
</li>
</ul>
</li>
<li>Other supervised learning considerations<ul>
<li>Features can be either numeric or categorical</li>
<li>Numeric features should be scaled (Z-scored)</li>
<li>Categorical features should be encoded (one-hot)</li>
</ul>
</li>
</ul>
<h2 id="Introducing-XGBoost">
<a class="anchor" href="#Introducing-XGBoost" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introducing XGBoost<a class="anchor-link" href="#Introducing-XGBoost"> </a>
</h2>
<ul>
<li>What is XGBoost? (eXtreme Gradient Boosting)<ul>
<li>Optimized gradient-boosting machine learning library</li>
<li>Originally written in C++</li>
<li>Has APIs in several languages:<ul>
<li>Python, R, Scala, Julia, Java</li>
</ul>
</li>
</ul>
</li>
<li>What makes XGBoost so popular?<ul>
<li>Speed and performance</li>
<li>Core algorithm is parallelizable</li>
<li>Consistently outperforms single-algorithm methods</li>
<li>State-of-the-art performance in many ML tasks</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="nn">xgb</span>

<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">'figure.figsize'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="XGBoost---Fit/Predict">
<a class="anchor" href="#XGBoost---Fit/Predict" aria-hidden="true"><span class="octicon octicon-link"></span></a>XGBoost - Fit/Predict<a class="anchor-link" href="#XGBoost---Fit/Predict"> </a>
</h3>
<p>It's time to create your first XGBoost model! As Sergey showed you in the video, you can use the scikit-learn <code>.fit()</code> / <code>.predict()</code> paradigm that you are already familiar to build your XGBoost models, as the xgboost library has a scikit-learn compatible API!</p>
<p>Here, you'll be working with churn data. This dataset contains imaginary data from a ride-sharing app with user behaviors over their first month of app usage in a set of imaginary cities as well as whether they used the service 5 months after sign-up.</p>
<p>Your goal is to use the first month's worth of data to predict whether the app's users will remain users of the service at the 5 month mark. This is a typical setup for a churn prediction problem. To do this, you'll split the data into training and test sets, fit a small xgboost model on the training set, and evaluate its performance on the test set by computing its accuracy.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">churn_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'./dataset/churn_data.csv'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Create arrays for the features and the target: X, y</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">churn_data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">churn_data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Create the training and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="c1"># Instantiate the XGBClassifier: xg_cl</span>
<span class="n">xg_cl</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="s1">'binary:logistic'</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="c1"># Fit the classifier to the training set</span>
<span class="n">xg_cl</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict the labels of the test set: preds</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">xg_cl</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Compute the accuracy: accuracy</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">preds</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">))</span> <span class="o">/</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"accuracy: </span><span class="si">%f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>accuracy: 0.758200
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-is-a-decision-tree?">
<a class="anchor" href="#What-is-a-decision-tree?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is a decision tree?<a class="anchor-link" href="#What-is-a-decision-tree?"> </a>
</h2>
<ul>
<li>Decision trees as base learners<ul>
<li>Base learner : Individual learning algorithm in an ensemble algorithm</li>
<li>Composed of a series of binary questions</li>
<li>Predictions happen at the "leaves" of the tree</li>
</ul>
</li>
<li>CART: Classification And Regression Trees<ul>
<li>Each leaf always contains a real-valued score</li>
<li>Can later be converted into categories</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Decision-trees">
<a class="anchor" href="#Decision-trees" aria-hidden="true"><span class="octicon octicon-link"></span></a>Decision trees<a class="anchor-link" href="#Decision-trees"> </a>
</h3>
<p>Your task in this exercise is to make a simple decision tree using scikit-learn's <code>DecisionTreeClassifier</code> on the breast cancer dataset.</p>
<p>This dataset contains numeric measurements of various dimensions of individual tumors (such as perimeter and texture) from breast biopsies and a single outcome value (the tumor is either malignant, or benign).</p>
<p>We've preloaded the dataset of samples (measurements) into <code>X</code> and the target values per tumor into <code>y</code>. Now, you have to split the complete dataset into training and testing sets, and then train a <code>DecisionTreeClassifier</code>. You'll specify a parameter called <code>max_depth</code>. Many other parameters can be modified within this model, and you can check all of them out here.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'./dataset/xgb_breast_X.csv'</span><span class="p">)</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'./dataset/xgb_breast_y.csv'</span><span class="p">)</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="c1"># Create the training and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="c1"># Instantiate the classifier: dt_clf_4</span>
<span class="n">dt_clf_4</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># Fit the classifier to the training set</span>
<span class="n">dt_clf_4</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict the labels of the test set: y_pred_4</span>
<span class="n">y_pred_4</span> <span class="o">=</span> <span class="n">dt_clf_4</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Compute the accuracy of the predictions: accuracy</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_pred_4</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">))</span> <span class="o">/</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Accuracy:"</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Accuracy: 0.9649122807017544
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-is-Boosting?">
<a class="anchor" href="#What-is-Boosting?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is Boosting?<a class="anchor-link" href="#What-is-Boosting?"> </a>
</h2>
<ul>
<li>Boosting overview<ul>
<li>Not a specific machine learning algorithm</li>
<li>Concept that can be applied to a set of machine learning models<ul>
<li>"Meta-algorithm"</li>
</ul>
</li>
<li>Ensemble meta-algorithm used to convert many weak learners into a strong learner</li>
</ul>
</li>
<li>Weak learners and strong learners<ul>
<li>Weak learner: ML algorithm that is slightly better than chance</li>
<li>Boosting converts a collection of weak learners into a strong learner</li>
<li>Strong learner: Any algorithm that can be tuned to achieve good performance.</li>
</ul>
</li>
<li>How boosting is accomplished?<ul>
<li>Iteratively learning a set of week models on subsets of the data</li>
<li>Weighting each weak prediction according to each weak learner's performance</li>
<li>Combine the weighted predictions to obtain a single weighted prediction</li>
<li>that is much better than the individual predictions themselves!</li>
</ul>
</li>
<li>Model evaluation through cross-validation<ul>
<li>Cross-validation: Robust method for estimating the performance of a model on unseen data</li>
<li>Generates many non-overlapping train/test splits on training data</li>
<li>Reports the average test set performance across all data splits</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Measuring-accuracy">
<a class="anchor" href="#Measuring-accuracy" aria-hidden="true"><span class="octicon octicon-link"></span></a>Measuring accuracy<a class="anchor-link" href="#Measuring-accuracy"> </a>
</h3>
<p>You'll now practice using XGBoost's learning API through its baked in cross-validation capabilities. As Sergey discussed in the previous video, XGBoost gets its lauded performance and efficiency gains by utilizing its own optimized data structure for datasets called a <code>DMatrix</code>.</p>
<p>In the previous exercise, the input datasets were converted into <code>DMatrix</code> data on the fly, but when you use the <code>xgboost</code> <code>cv</code> object, you have to first explicitly convert your data into a <code>DMatrix</code>. So, that's what you will do here before running cross-validation on <code>churn_data</code>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">churn_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'./dataset/churn_data.csv'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">churn_data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">churn_data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Create the DMatrix from X and y: churn_dmatrix</span>
<span class="n">churn_dmatrix</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Create the parameter dictionary: params</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'objective'</span><span class="p">:</span><span class="s2">"reg:logistic"</span><span class="p">,</span> <span class="s2">"max_depth"</span><span class="p">:</span><span class="mi">3</span><span class="p">}</span>

<span class="c1"># Perform cross-validation: cv_results</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span><span class="n">dtrain</span><span class="o">=</span><span class="n">churn_dmatrix</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
                   <span class="n">nfold</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                   <span class="n">metrics</span><span class="o">=</span><span class="s2">"error"</span><span class="p">,</span> <span class="n">as_pandas</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="c1"># Pint cv_results</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cv_results</span><span class="p">)</span>

<span class="c1"># Print the accuracy</span>
<span class="nb">print</span><span class="p">(((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">cv_results</span><span class="p">[</span><span class="s1">'test-error-mean'</span><span class="p">])</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>   train-error-mean  train-error-std  test-error-mean  test-error-std
0           0.28232         0.002366          0.28378        0.001932
1           0.26951         0.001855          0.27190        0.001932
2           0.25605         0.003213          0.25798        0.003963
3           0.25090         0.001845          0.25434        0.003827
4           0.24654         0.001981          0.24852        0.000934
0.75148
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>cv_results</code> stores the training and test mean and standard deviation of the error per boosting round (tree built) as a DataFrame. From <code>cv_results</code>, the final round <code>'test-error-mean'</code> is extracted and converted into an accuracy, where accuracy is <code>1-error</code>. The final accuracy of around 75% is an improvement from earlier!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Measuring-AUC">
<a class="anchor" href="#Measuring-AUC" aria-hidden="true"><span class="octicon octicon-link"></span></a>Measuring AUC<a class="anchor-link" href="#Measuring-AUC"> </a>
</h3>
<p>Now that you've used cross-validation to compute average out-of-sample accuracy (after converting from an error), it's very easy to compute any other metric you might be interested in. All you have to do is pass it (or a list of metrics) in as an argument to the metrics parameter of <code>xgb.cv()</code>.</p>
<p>Your job in this exercise is to compute another common metric used in binary classification - the area under the curve (<code>"auc"</code>).</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">cv_results</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span><span class="n">dtrain</span><span class="o">=</span><span class="n">churn_dmatrix</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
                    <span class="n">nfold</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                    <span class="n">metrics</span><span class="o">=</span><span class="s2">"auc"</span><span class="p">,</span> <span class="n">as_pandas</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="c1"># Print cv_results</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cv_results</span><span class="p">)</span>

<span class="c1"># Print the AUC</span>
<span class="nb">print</span><span class="p">((</span><span class="n">cv_results</span><span class="p">[</span><span class="s2">"test-auc-mean"</span><span class="p">])</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>   train-auc-mean  train-auc-std  test-auc-mean  test-auc-std
0        0.768893       0.001544       0.767863      0.002820
1        0.790864       0.006758       0.789157      0.006846
2        0.815872       0.003900       0.814476      0.005997
3        0.822959       0.002018       0.821682      0.003912
4        0.827528       0.000769       0.826191      0.001937
0.826191
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>An AUC of 0.84 is quite strong. As you have seen, XGBoost's learning API makes it very easy to compute any metric you may be interested in. In Chapter 3, you'll learn about techniques to fine-tune your XGBoost models to improve their performance even further. For now, it's time to learn a little about exactly when to use XGBoost.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="When-should-I-use-XGBoost?">
<a class="anchor" href="#When-should-I-use-XGBoost?" aria-hidden="true"><span class="octicon octicon-link"></span></a>When should I use XGBoost?<a class="anchor-link" href="#When-should-I-use-XGBoost?"> </a>
</h2>
<ul>
<li>When to use XGBoost<ul>
<li>You have a large number of training samples<ul>
<li>Greater than 1000 training samples and less 100 features</li>
<li>The number of features &lt; number of training samples</li>
</ul>
</li>
<li>You have a mixture of categorical and numeric features<ul>
<li>Or just numeric features</li>
</ul>
</li>
</ul>
</li>
<li>When to NOT use XGBoost<ul>
<li>Image recognition</li>
<li>Computer vision</li>
<li>Natural language processing and understanding problems</li>
<li>When the number of training samples is significantly smaller than the number of features</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="goodboychan/goodboychan.github.io"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/python/datacamp/machine_learning/2020/07/06/03-Classification-with-XGBoost.html" hidden></a>
</article>

      </section>
      <footer>
        
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
  </body>
</html>
